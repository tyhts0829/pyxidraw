"""
CLI ã‚³ãƒãƒ³ãƒ‰å®Ÿè£…

å„CLIã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…ã‚’è²¬å‹™åˆ¥ã«åˆ†é›¢ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import json
import sys
import logging
from pathlib import Path
from typing import Optional, List, Dict, Any

from benchmarks.core.config import BenchmarkConfigManager
from benchmarks.core.runner import UnifiedBenchmarkRunner
from benchmarks.core.validator import BenchmarkValidator, BenchmarkResultAnalyzer
from benchmarks.benchmark_result_manager import BenchmarkResultManager
from benchmarks.core.types import BenchmarkResult


class CommandExecutor:
    """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, args):
        self.args = args
        self.verbose = getattr(args, 'verbose', False)
        self.logger = logging.getLogger(__name__)
    
    def load_config(self):
        """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        config_manager = BenchmarkConfigManager(self.args.config)
        config = config_manager.load_config()
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§è¨­å®šã‚’ä¸Šæ›¸ã
        if hasattr(self.args, 'output_dir') and self.args.output_dir:
            config.output_dir = self.args.output_dir
        
        if hasattr(self.args, 'parallel') and self.args.parallel:
            config.parallel = True
        
        if hasattr(self.args, 'workers') and self.args.workers:
            config.max_workers = self.args.workers
        
        if hasattr(self.args, 'warmup') and self.args.warmup:
            config.warmup_runs = self.args.warmup
        
        if hasattr(self.args, 'runs') and self.args.runs:
            config.measurement_runs = self.args.runs
        
        if hasattr(self.args, 'timeout') and self.args.timeout:
            config.timeout_seconds = self.args.timeout
        
        if hasattr(self.args, 'no_charts') and self.args.no_charts:
            config.generate_charts = False
        
        # è¨­å®šã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        config_manager.validate_config(config)
        
        return config
    
    def print_verbose(self, message: str):
        """è©³ç´°å‡ºåŠ›"""
        if self.verbose:
            self.logger.debug(message)


class RunCommand(CommandExecutor):
    """runã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """runã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        config = self.load_config()
        self.print_verbose(f"è¨­å®š: {config}")
        
        # ãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆ
        runner = UnifiedBenchmarkRunner(config)
        
        try:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            results = self._run_benchmarks(runner)
            
            if not results:
                self.logger.warning("No benchmarks were executed")
                return 1
            
            # çµæœã®ä¿å­˜
            self._save_results(results, config)
            
            # çµæœã®åˆ†æã¨è¡¨ç¤º
            self._analyze_and_display_results(results)
            
            return 0
            
        except Exception as e:
            self.logger.exception("Error running benchmarks: %s", e)
            return 1
    
    def _run_benchmarks(self, runner: UnifiedBenchmarkRunner) -> List[BenchmarkResult]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        # --from-file ã®èª­ã¿è¾¼ã¿
        targets_from_file: List[str] = []
        if getattr(self.args, 'from_file', None):
            try:
                with open(self.args.from_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        targets_from_file = [str(x) for x in data]
                    elif isinstance(data, dict) and 'targets' in data:
                        targets_from_file = [str(x) for x in data['targets']]
            except Exception as e:
                self.logger.warning("Failed to read --from-file: %s", e)

        # å„ªå…ˆé †: --target / --from-file / all
        if getattr(self.args, 'target', None):
            names: List[str] = self.args.target
        elif targets_from_file:
            names = targets_from_file
        else:
            names = []

        # ã‚¹ã‚­ãƒƒãƒ—æŒ‡å®š
        skip: set[str] = set(getattr(self.args, 'skip', []) or [])

        if names:
            # æŒ‡å®šã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‹ã‚‰ã‚¹ã‚­ãƒƒãƒ—ã‚’é™¤å¤–
            names = [n for n in names if n not in skip]
            results_dict = runner.run_specific_targets(names)
            return list(results_dict.values())
        else:
            # å…¨å®Ÿè¡Œ â†’ ã‚¹ã‚­ãƒƒãƒ—ã¯å¾Œæ®µã§ãƒ•ã‚£ãƒ«ã‚¿
            all_targets = runner.plugin_manager.get_all_targets()
            # ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã‚’é™¤å¤–
            for plugin, targets in all_targets.items():
                all_targets[plugin] = [t for t in targets if t.name not in skip and f"{plugin}.{t.name}" not in skip]
            # å®Ÿè¡Œ
            results: List[BenchmarkResult] = []
            for plugin, targets in all_targets.items():
                if not targets:
                    continue
                # ä¸€æ™‚ãƒ©ãƒ³ãƒŠãƒ¼ã§é€æ¬¡å®Ÿè¡Œï¼ˆæ—¢å­˜run_all_benchmarksã¯å†åˆ©ç”¨ã—ãªã„ï¼‰
                # ãŸã ã—æ—¢å­˜ã®run_all_benchmarksã§ã‚‚ã‚¹ã‚­ãƒƒãƒ—å½±éŸ¿ã‚’å‡ºã™ã«ã¯å†…éƒ¨å¤‰æ›´ãŒå¿…è¦ãªãŸã‚ç°¡æ˜“å®Ÿè£…
                for t in targets:
                    res = runner.benchmark_target(t)
                    results.append(res)
            return results
    
    def _save_results(self, results: List[BenchmarkResult], config):
        """çµæœã‚’ä¿å­˜"""
        if not getattr(self.args, 'no_save', False):
            # ãƒªã‚¹ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            results_dict = {result.target_name: result for result in results}
            result_manager = BenchmarkResultManager(str(config.output_dir))
            saved_file = result_manager.save_results(results_dict)
            self.logger.info("Results saved to: %s", saved_file)
            # å¤±æ•—ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ›¸ãå‡ºã—
            failed = [r.target_name for r in results if not r.success]
            try:
                fail_path = Path(config.output_dir) / "failed_targets.json"
                with open(fail_path, 'w', encoding='utf-8') as f:
                    json.dump(failed, f, indent=2, ensure_ascii=False)
                self.logger.info("Failed targets saved to: %s", fail_path)
            except Exception as e:
                self.logger.warning("Failed to save failed_targets.json: %s", e)
    
    def _analyze_and_display_results(self, results: List[BenchmarkResult]):
        """çµæœã‚’åˆ†æã—ã¦è¡¨ç¤º"""
        try:
            analyzer = BenchmarkResultAnalyzer()
            # ãƒªã‚¹ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            results_dict = {result.target_name: result for result in results}
            analysis = analyzer.analyze_results(results_dict)
            
            # è¦ç´„ã‚’è¡¨ç¤º
            self._display_summary(analysis["summary"])
            
            # æ¤œè¨¼çµæœã‚’è¡¨ç¤º
            validation = analysis["validation"]
            self._display_validation(validation)
        except Exception as e:
            self.logger.exception("Error in result analysis: %s", e)
            import traceback
            traceback.print_exc()
            raise
    
    def _display_summary(self, summary: Dict[str, Any]):
        """è¦ç´„ã‚’è¡¨ç¤º"""
        print(f"\n=== BENCHMARK SUMMARY ===")
        print(f"Total modules: {summary['total_modules']}")
        print(f"Successful: {summary['successful']}")
        print(f"Failed: {summary['failed']}")
        print(f"Success rate: {summary['success_rate']:.1%}")
        
        if summary['successful'] > 0:
            print(f"Fastest time: {summary['fastest_time']*1000:.3f}ms")
            print(f"Slowest time: {summary['slowest_time']*1000:.3f}ms")
            print(f"Average time: {summary['average_time']*1000:.3f}ms")
    
    def _display_validation(self, validation: Dict[str, Any]):
        """æ¤œè¨¼çµæœã‚’è¡¨ç¤º"""
        if validation["errors"]:
            print(f"\nâš ï¸  Validation errors: {len(validation['errors'])}")
            for error in validation["errors"][:5]:  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
                print(f"  - {error}")
            if len(validation["errors"]) > 5:
                print(f"  ... and {len(validation['errors']) - 5} more")


class ListCommand(CommandExecutor):
    """listã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """listã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            config = self.load_config()
            runner = UnifiedBenchmarkRunner(config)
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å–å¾—ï¼ˆã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ã®ãŸã‚è©³ç´°å–å¾—ï¼‰
            all_targets = runner.plugin_manager.get_all_targets()
            filtered: List[str] = []
            for plugin_name, tlist in all_targets.items():
                for t in tlist:
                    fq = f"{plugin_name}.{t.name}"
                    filtered.append(fq)
            targets = filtered
            # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿
            if getattr(self.args, 'plugin', None):
                targets = [t for t in targets if t.startswith(self.args.plugin)]
            # ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿
            if getattr(self.args, 'tag', None):
                want_tags = set(self.args.tag)
                # å†å–å¾—ã—ã¦ã‚¿ã‚°ã§è½ã¨ã™
                tagged: List[str] = []
                for plugin_name, tlist in all_targets.items():
                    for t in tlist:
                        t_tags = set(getattr(t, 'tags', []) or [])
                        if want_tags.issubset(t_tags):
                            tagged.append(f"{plugin_name}.{t.name}")
                targets = [t for t in targets if t in tagged]
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥å‡ºåŠ›
            format_type = getattr(self.args, 'format', 'table')
            self._display_targets(targets, format_type)
            
            return 0
            
        except Exception as e:
            self.logger.exception("Error listing targets: %s", e)
            return 1
    
    def _display_targets(self, targets: List[str], format_type: str):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡¨ç¤º"""
        if format_type == "json":
            print(json.dumps(targets, indent=2))
        elif format_type == "yaml":
            print("targets:")
            for target in targets:
                print(f"  - {target}")
        else:  # table
            print("Available benchmark targets:")
            for target in targets:
                print(f"  {target}")


class ValidateCommand(CommandExecutor):
    """validateã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """validateã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            results_file = self.args.results_file
            
            if not results_file.exists():
                self.logger.error("Results file not found: %s", results_file)
                return 1
            
            # çµæœã‚’èª­ã¿è¾¼ã¿
            with open(results_file, 'r') as f:
                results_data = json.load(f)
            
            # æ¤œè¨¼å®Ÿè¡Œ
            validator = BenchmarkValidator()
            validation_result = validator.validate_multiple_results(results_data)
            
            # ValidationResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
            validation_dict = {
                "is_valid": validation_result.is_valid,
                "errors": validation_result.errors,
                "warnings": validation_result.warnings,
                "metrics": validation_result.metrics
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            if hasattr(self.args, 'report') and self.args.report:
                self._generate_validation_report(validation_dict, self.args.report)
            else:
                self._display_validation_result(validation_dict)
            
            return 0 if validation_result.is_valid else 1
            
        except Exception as e:
            self.logger.exception("Error validating results: %s", e)
            return 1
    
    def _display_validation_result(self, result: Dict[str, Any]):
        """æ¤œè¨¼çµæœã‚’è¡¨ç¤º"""
        print(f"Validation result: {'PASS' if result['is_valid'] else 'FAIL'}")
        
        if result["errors"]:
            print(f"\nErrors ({len(result['errors'])}):")
            for error in result["errors"]:
                print(f"  - {error}")
        
        if result["warnings"]:
            print(f"\nWarnings ({len(result['warnings'])}):")
            for warning in result["warnings"]:
                print(f"  - {warning}")
    
    def _generate_validation_report(self, result: Dict[str, Any], report_path: Path):
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_content = {
            "validation_status": "PASS" if result["is_valid"] else "FAIL",
            "timestamp": result.get("timestamp"),
            "errors": result["errors"],
            "warnings": result["warnings"],
            "statistics": result.get("statistics", {})
        }
        
        with open(report_path, 'w') as f:
            json.dump(report_content, f, indent=2)
        
        self.logger.info("Validation report saved to: %s", report_path)


class CompareCommand(CommandExecutor):
    """compareã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """compareã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            baseline_file = self.args.baseline
            current_file = self.args.current
            
            if not baseline_file.exists():
                self.logger.error("Baseline file not found: %s", baseline_file)
                return 1
            
            if not current_file.exists():
                self.logger.error("Current file not found: %s", current_file)
                return 1
            
            # çµæœã‚’èª­ã¿è¾¼ã¿
            with open(baseline_file, 'r') as f:
                baseline_data = json.load(f)
            
            with open(current_file, 'r') as f:
                current_data = json.load(f)
            
            # ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¿å­˜çµæœã« 'tags' ãŒã‚ã‚‹å‰æã€‚ãªã‘ã‚Œã°é€šã•ãªã„ï¼‰
            if getattr(self.args, 'tag', None):
                need = set(self.args.tag)
                baseline_data = {k: v for k, v in baseline_data.items() if isinstance(v, dict) and need.issubset(set(v.get('tags', []) or []))}
                current_data = {k: v for k, v in current_data.items() if isinstance(v, dict) and need.issubset(set(v.get('tags', []) or []))}

            # æ¯”è¼ƒå®Ÿè¡Œï¼ˆè¾æ›¸åŒå£«ã®è»½é‡æ¯”è¼ƒï¼‰
            comparison = self._compare_dicts(baseline_data, current_data)

            # çµæœè¡¨ç¤º
            self._display_comparison_result(comparison)

            # å›å¸°æ¤œå‡ºï¼ˆã‚¿ã‚°/ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ¥é–¾å€¤ã¨çµ¶å¯¾å€¤é–¾å€¤ã‚’è€ƒæ…®ï¼‰
            cfg = self.load_config()
            threshold_default = getattr(self.args, 'regression_threshold', -0.1)
            abs_thr = getattr(self.args, 'abs_threshold', 0.0) or 0.0
            regressions = []
            for c in comparison:
                tgt = c["target"]
                change = c.get("performance_change", 0.0)
                abs_diff = c.get("abs_diff", 0.0)
                if abs_diff < abs_thr:
                    continue  # ç„¡è¦–ï¼ˆãƒã‚¤ã‚ºï¼‰
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ/ã‚¿ã‚°åˆ¥ã®ã—ãã„å€¤ãŒã‚ã‚Œã°ä¸Šæ›¸ã
                thr = cfg.regression_threshold_by_target.get(tgt, threshold_default)
                # ã‚¿ã‚°ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®currentå´ã‚’å‚ç…§ï¼ˆãªã‘ã‚Œã°baselineï¼‰
                tags = []
                if tgt in current_data and isinstance(current_data[tgt], dict):
                    tags = current_data[tgt].get('tags', []) or []
                elif tgt in baseline_data and isinstance(baseline_data[tgt], dict):
                    tags = baseline_data[tgt].get('tags', []) or []
                for t in tags:
                    if t in cfg.regression_threshold_by_tag:
                        thr = cfg.regression_threshold_by_tag[t]
                        break
                if change < thr:
                    regressions.append(c)
            
            if regressions:
                self.logger.warning("âš ï¸  Performance regressions detected: %d", len(regressions))
                return 1
            
            return 0
            
        except Exception as e:
            self.logger.exception("Error comparing results: %s", e)
            return 1
    
    def _display_comparison_result(self, comparison: List[Dict[str, Any]]):
        """æ¯”è¼ƒçµæœã‚’è¡¨ç¤º"""
        print("=== BENCHMARK COMPARISON ===")
        
        for result in comparison:
            target = result["target"]
            change = result.get("performance_change", 0)
            abs_diff = result.get("abs_diff", 0.0)
            
            if change > 0.05:  # 5%ä»¥ä¸Šã®æ”¹å–„
                status = "ğŸš€ IMPROVED"
            elif change < -0.05:  # 5%ä»¥ä¸Šã®åŠ£åŒ–
                status = "ğŸŒ REGRESSED"
            else:
                status = "â¡ï¸  UNCHANGED"
            
            print(f"{target}: {status} ({change:+.1%}, Î”={abs_diff*1000:.3f} ms)")

    def _compare_dicts(self, baseline: Dict[str, Any], current: Dict[str, Any]) -> List[Dict[str, Any]]:
        """JSONè¾æ›¸ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿çµæœï¼‰åŒå£«ã®æ¯”è¼ƒã‚’è¡Œã„ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã”ã¨ã®å·®åˆ†ã‚’è¿”ã™ã€‚"""
        results: List[Dict[str, Any]] = []
        keys = sorted(set(baseline.keys()) & set(current.keys()))
        for k in keys:
            b = baseline[k]
            c = current[k]
            # å¹³å‡æ™‚é–“ã®æŠ½å‡ºï¼ˆæ–°/æ—§ã®ä¸¡å½¢å¼ã«å¯¾å¿œï¼‰
            def _avg(x: Any) -> float:
                # æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿å¯¾å¿œ
                if isinstance(x, dict) and isinstance(x.get('timing_data'), dict):
                    return float(x['timing_data'].get('average_time', 0.0) or 0.0)
                return 0.0
            b_avg = _avg(b)
            c_avg = _avg(c)
            if b_avg == 0:
                change = float('inf') if c_avg == 0 else -float('inf')
            else:
                change = (b_avg - c_avg) / b_avg
            results.append({
                "target": k,
                "baseline_avg": b_avg,
                "current_avg": c_avg,
                "performance_change": change,
                "abs_diff": abs(c_avg - b_avg),
            })
        return results


class ConfigCommand(CommandExecutor):
    """configã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """configã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            action = getattr(self.args, 'config_action', None)
            
            if action == 'template':
                return self._create_template()
            elif action == 'show':
                return self._show_config()
            else:
                self.logger.error("No config action specified")
                return 1
                
        except Exception as e:
            self.logger.exception("Error in config command: %s", e)
            return 1
    
    def _create_template(self) -> int:
        """è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ"""
        output_file = self.args.output_file
        config_manager = BenchmarkConfigManager()
        
        template = config_manager.generate_template()
        
        with open(output_file, 'w') as f:
            f.write(template)
        
        self.logger.info("Configuration template created: %s", output_file)
        return 0
    
    def _show_config(self) -> int:
        """ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º"""
        config = self.load_config()
        self.logger.info("Current configuration:")
        print(json.dumps(config.__dict__, indent=2, default=str))
        return 0


# ã‚³ãƒãƒ³ãƒ‰ç™»éŒ²ãƒãƒƒãƒ—
COMMAND_MAP = {
    'run': RunCommand,
    'list': ListCommand,
    'validate': ValidateCommand,
    'compare': CompareCommand,
    'config': ConfigCommand,
}


def execute_command(command: str, args) -> int:
    """æŒ‡å®šã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
    if command not in COMMAND_MAP:
        logging.getLogger(__name__).error("Unknown command: %s", command)
        return 1
    
    command_class = COMMAND_MAP[command]
    executor = command_class(args)
    return executor.execute()
